from fastapi import APIRouter, HTTPException, Header
from fastapi.responses import FileResponse, StreamingResponse
from typing import Optional
import logging
import os
import json
import io
import zipfile
from utils.file_handler import file_handler
from models.schemas import ExportRequest, ExportResponse

logger = logging.getLogger(__name__)

router = APIRouter()


@router.get("/download/{session_id}/{filename}")
async def download_file(session_id: str, filename: str):
    """
    Download a file from the session directory.
    """
    try:
        file_path = os.path.join(file_handler.upload_dir, session_id, filename)
        
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail="File not found")
            
        return FileResponse(
            path=file_path,
            filename=filename,
            media_type='application/octet-stream'
        )
        
    except Exception as e:
        logger.error(f"Download error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/", response_model=ExportResponse)
async def export_data(
    request: ExportRequest,
    session_id: Optional[str] = Header(None, alias="X-Session-ID")
):
    """
    Export data in multiple formats (CSV, Excel, Python script, Jupyter notebook, JSON).
    """
    if not session_id:
        session_id = request.session_id
        
    if not session_id:
        raise HTTPException(status_code=400, detail="Session ID required")
        
    try:
        session_dir = os.path.join(file_handler.upload_dir, session_id)
        files_created = {}
        
        # Load the most processed dataset
        df = None
        for filename in ["engineered.csv", "cleaned.csv", "original.csv"]:
            try:
                df = file_handler.load_dataframe(session_id, filename)
                break
            except FileNotFoundError:
                continue
                
        if df is None:
            raise HTTPException(status_code=404, detail="No dataset found")
        
        # Export in requested formats
        for fmt in request.formats:
            if fmt == "csv":
                csv_path = os.path.join(session_dir, "export.csv")
                df.to_csv(csv_path, index=False)
                files_created["csv"] = f"/api/export/download/{session_id}/export.csv"
                
            elif fmt == "excel":
                excel_path = os.path.join(session_dir, "export.xlsx")
                df.to_excel(excel_path, index=False, sheet_name="Data")
                files_created["excel"] = f"/api/export/download/{session_id}/export.xlsx"
                
            elif fmt == "json":
                json_path = os.path.join(session_dir, "export.json")
                # Replace NaN with None for valid JSON
                df_clean = df.replace({float('nan'): None})
                with open(json_path, 'w') as f:
                    json.dump(df_clean.to_dict(orient='records'), f, indent=2)
                files_created["json"] = f"/api/export/download/{session_id}/export.json"
                
            elif fmt == "python":
                # Generate Python script
                py_path = os.path.join(session_dir, "data_analysis.py")
                script = _generate_python_script(session_id)
                with open(py_path, 'w') as f:
                    f.write(script)
                files_created["python"] = f"/api/export/download/{session_id}/data_analysis.py"
                
            elif fmt == "notebook":
                # Generate Jupyter notebook
                nb_path = os.path.join(session_dir, "data_analysis.ipynb")
                notebook = _generate_jupyter_notebook(session_id)
                with open(nb_path, 'w') as f:
                    json.dump(notebook, f, indent=2)
                files_created["notebook"] = f"/api/export/download/{session_id}/data_analysis.ipynb"
        
        # Include original if requested
        if request.include_original:
            try:
                original_path = os.path.join(session_dir, "original.csv")
                if os.path.exists(original_path):
                    files_created["original"] = f"/api/export/download/{session_id}/original.csv"
            except:
                pass
        
        logger.info(f"Export completed for session: {session_id}")
        
        return ExportResponse(
            session_id=session_id,
            files=files_created,
            summary=f"Exported {len(files_created)} file(s) in {len(request.formats)} format(s)"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Export error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


def _generate_python_script(session_id: str) -> str:
    """Generate a Python script for reproducing the analysis"""
    return f"""#!/usr/bin/env python3
\"\"\"
Data Analysis Script
Generated by DIDA - Domain-Aware Intelligent Data Scientist Agent
Session: {session_id}
\"\"\"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

# Load data
print("Loading dataset...")
df = pd.read_csv('export.csv')

# Basic information
print(f"\\nDataset shape: {{df.shape}}")
print(f"\\nColumn types:\\n{{df.dtypes}}")
print(f"\\nMissing values:\\n{{df.isnull().sum()}}")

# Statistical summary
print(f"\\nStatistical Summary:\\n{{df.describe()}}")

# Sample rows
print(f"\\nFirst 5 rows:\\n{{df.head()}}")

# You can add your custom analysis below
# Example: df.groupby('column').mean()
# Example: plt.figure(); df['column'].hist(); plt.show()

print("\\nAnalysis complete!")
"""


def _generate_jupyter_notebook(session_id: str) -> dict:
    """Generate a Jupyter notebook for interactive analysis"""
    return {
        "cells": [
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": [
                    "# Data Analysis Notebook\n",
                    f"Generated by DIDA - Session: {session_id}\n",
                    "\n",
                    "This notebook provides a starting point for analyzing your data."
                ]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "import pandas as pd\n",
                    "import numpy as np\n",
                    "import matplotlib.pyplot as plt\n",
                    "import seaborn as sns\n",
                    "from sklearn.model_selection import train_test_split\n",
                    "from sklearn.preprocessing import StandardScaler\n",
                    "\n",
                    "# Set plotting style\n",
                    "sns.set_style('whitegrid')\n",
                    "plt.rcParams['figure.figsize'] = (12, 6)"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Load Data"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Load the dataset\n",
                    "df = pd.read_csv('export.csv')\n",
                    "print(f'Dataset shape: {df.shape}')\n",
                    "df.head()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Data Overview"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Basic information\n",
                    "print('Column Types:')\n",
                    "print(df.dtypes)\n",
                    "print('\\nMissing Values:')\n",
                    "print(df.isnull().sum())\n",
                    "print('\\nStatistical Summary:')\n",
                    "df.describe()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Exploratory Data Analysis"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Add your analysis here\n",
                    "# Example: correlation matrix\n",
                    "# numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
                    "# plt.figure(figsize=(10, 8))\n",
                    "# sns.heatmap(df[numeric_cols].corr(), annot=True, cmap='coolwarm')\n",
                    "# plt.title('Correlation Matrix')\n",
                    "# plt.show()"
                ]
            },
            {
                "cell_type": "markdown",
                "metadata": {},
                "source": ["## Further Analysis"]
            },
            {
                "cell_type": "code",
                "execution_count": None,
                "metadata": {},
                "source": [
                    "# Add your custom analysis and visualizations here"
                ]
            }
        ],
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "name": "python",
                "version": "3.9.0"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }
